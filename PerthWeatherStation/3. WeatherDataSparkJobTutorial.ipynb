{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial\n",
    "A. This tutorial will show you how to \n",
    "    1. Create jobs\n",
    "    2. Run instances of a job\n",
    "    3. Run a job on a schedule\n",
    "    4. View data in sql directly\n",
    "B. Requirements\n",
    "    1. Using the linear regression calculated in the Magics tutorial develop some spark code in a spark context that\n",
    "        - takes the joined output table generated in the Magics tutorial (D3S_Training_%Name%_PerthMaxTemps)\n",
    "        - filter the table to a particular day\n",
    "        - add a new column to the table named PerthEstimate which stores the result of the regression\n",
    "        - calculate the day as the \n",
    "            - days=floor of the ((the current datetime minus recent datetime in minutes) divided by 5)\n",
    "            - day=datetime(2014,1,1)+timedelta(days=days)\n",
    "    2. Create a table definition to store the output of the code above in a SQL store\n",
    "    3. Take the script developed above and create a job with D3S_Training_%Name%_PerthMaxTemps as the input table and the new table definition as the output\n",
    "    4. Run a single instance of the job\n",
    "    5. Run the job on a 5 minute schedule. This will cause a new days data to be written out every 5 minutes\n",
    "    6. View the resulting data in the SQL data store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuro_python.neuro_compute import spark_manager as spm\n",
    "from neuro_python.neuro_data import schema_manager as sm\n",
    "from neuro_python.neuro_data import sql_commands as sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start the default cluster and ensure it's running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spm.start_cluster()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spm.list_clusters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create context once cluster is in running state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spm.create_context('TrainingContext')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Build the job script and test it using magics\n",
    "Change the data store names to match yours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark_import_table\n",
    "import_table('df_PerthMaxTemps','DataLakeName','D3S_Training_Lee_PerthMaxTemps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark_sql\n",
    "select *, (Airport_MaxTemp * 0.668860401751 + Gosnells_MaxTemp * 0.120407996316 + \n",
    "           Swanbourne_MaxTemp * 0.145763508482 + Hillarys_MaxTemp * 0.0559960642181 + 0.14003979078607637) as PerthEstimate_MaxTemp\n",
    "from df_PerthMaxTemps\n",
    "where\n",
    "    Year = 2014\n",
    "    and Month = 1\n",
    "    and Day = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark\n",
    "year = 2014\n",
    "month = 1\n",
    "day = 1\n",
    "df_PerthMaxTemps.registerTempTable('df_PerthMaxTemps')\n",
    "df_Estimate = spark.sql(\"\"\"\n",
    "select *, (Airport_MaxTemp * 0.668860401751 + Gosnells_MaxTemp * 0.120407996316 + \n",
    "           Swanbourne_MaxTemp * 0.145763508482 + Hillarys_MaxTemp * 0.0559960642181 + 0.14003979078607637) as PerthEstimate_MaxTemp\n",
    "from df_PerthMaxTemps\n",
    "where\n",
    "    Year = %s\n",
    "    and Month = %s\n",
    "    and Day = %s\"\"\"%(year, month, day))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%spark_pandas -df df_Estimate.limit(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark\n",
    "import datetime\n",
    "import math\n",
    "current_date=datetime.datetime.utcnow()\n",
    "startDateTime = datetime.datetime(2019,10,29,2,45)\n",
    "dataStartDateTime = datetime.datetime(2014,1,1)\n",
    "diff = (current_date - startDateTime).total_seconds()\n",
    "iterations = math.floor(diff/(60*5))\n",
    "estimatedDay = dataStartDateTime + datetime.timedelta(days=iterations)\n",
    "year = estimatedDay.year\n",
    "month = estimatedDay.month\n",
    "day = estimatedDay.day\n",
    "df_PerthMaxTemps.registerTempTable('df_PerthMaxTemps')\n",
    "df_Estimate = spark.sql(\"\"\"\n",
    "select *, (Airport_MaxTemp * 0.668860401751 + Gosnells_MaxTemp * 0.120407996316 + \n",
    "           Swanbourne_MaxTemp * 0.145763508482 + Hillarys_MaxTemp * 0.0559960642181 + 0.14003979078607637) as PerthEstimate_MaxTemp\n",
    "from df_PerthMaxTemps\n",
    "where\n",
    "    Year = %s\n",
    "    and Month = %s\n",
    "    and Day = %s\"\"\"%(year, month, day))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%spark_pandas -df df_Estimate.limit(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create output table in a SQL store\n",
    "Change the data store name to match yours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols=[sm.column_definition('Year','Int'),\n",
    "     sm.column_definition('Month','Int'),\n",
    "     sm.column_definition('Day','Int'),\n",
    "     sm.column_definition('Airport_MaxTemp','Double'),\n",
    "     sm.column_definition('Gosnells_MaxTemp','Double'),\n",
    "     sm.column_definition('Swanbourne_MaxTemp','Double'),\n",
    "     sm.column_definition('Perth_MaxTemp','Double'),\n",
    "     sm.column_definition('Hillarys_MaxTemp','Double'),\n",
    "     sm.column_definition('PerthEstimate_MaxTemp','Double')]\n",
    "table_def=sm.table_definition(cols,'Processed',file_type='delta')\n",
    "sm.create_table('SqlStoreName','D3S_Training_Lee_PerthMaxTempsEstimate',table_def)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Submit the job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?spm.submit_job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyscript = '''\n",
    "import datetime\n",
    "import math\n",
    "current_date=datetime.datetime.utcnow()\n",
    "startDateTime = datetime.datetime(2019,10,29,2,45)\n",
    "dataStartDateTime = datetime.datetime(2014,1,1)\n",
    "diff = (current_date - startDateTime).total_seconds()\n",
    "iterations = math.floor(diff/(60*5))\n",
    "estimatedDay = dataStartDateTime + datetime.timedelta(days=iterations)\n",
    "year = estimatedDay.year\n",
    "month = estimatedDay.month\n",
    "day = estimatedDay.day\n",
    "df_PerthMaxTemps.registerTempTable('df_PerthMaxTemps')\n",
    "df_Estimate = spark.sql(\"\"\"\n",
    "select *, (Airport_MaxTemp * 0.668860401751 + Gosnells_MaxTemp * 0.120407996316 + \n",
    "           Swanbourne_MaxTemp * 0.145763508482 + Hillarys_MaxTemp * 0.0559960642181 + 0.14003979078607637) as PerthEstimate_MaxTemp\n",
    "from df_PerthMaxTemps\n",
    "where\n",
    "    Year = %s\n",
    "    and Month = %s\n",
    "    and Day = %s\"\"\"%(year, month, day))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "itable=spm.import_table('df_PerthMaxTemps','DataLakeName','D3S_Training_Lee_PerthMaxTemps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "etable=spm.export_table('df_Estimate','SqlStoreName','D3S_Training_Lee_PerthMaxTempsEstimate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job = spm.submit_job('D3S_Training_Lee_EstimatePerthMaxTemp',pyscript,import_tables=[itable],export_tables=[etable])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## See your job in the list of jobs and inspect the detail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spm.list_jobs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spm.get_job_details(job['JobId'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Run a single instance of the job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?spm.run_job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run=spm.run_job(job['JobId'],'Test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### View the status of the run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spm.list_runs(job['JobId'],run_id=run['RunId'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Run the job on a schedule\n",
    "Crontab is used for the cron expression\n",
    "https://crontab.guru/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?spm.run_schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schedule=spm.run_schedule(job['JobId'],'TestSchedule','*/5 * * * *')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### View the status of run triggered by the schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spm.list_runs(job['JobId'],schedule_id=schedule['ScheduleId'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. View the results in the sql table\n",
    "Change the data store name to match yours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql -sn SqlStoreName\n",
    "select top 100 *\n",
    "from D3S_Training_Lee_PerthMaxTempsEstimate"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
