{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial\n",
    "A. This tutorial will show you how to \n",
    "    1. Read data into a spark context\n",
    "    2. Perform some transformation\n",
    "    3. Calculate some Statistics\n",
    "    4. Plot and view results\n",
    "    5. Build a model to estimate values\n",
    "    6. Export and save generated data\n",
    "B. Requirements\n",
    "    1. Load the tables into a spark context\n",
    "        - From Data Lake Store\n",
    "            - D3S_Training_9021_MaxTemp\n",
    "            - D3S_Training_9106_MaxTemp\n",
    "            - D3S_Training_9215_MaxTemp\n",
    "            - D3S_Training_9225_MaxTemp\n",
    "            - D3S_Training_9265_MaxTemp\n",
    "        - From SQL Store\n",
    "            - D3S_Training_WeatherStations\n",
    "    2. View the top of each of the tables\n",
    "    3. Find the overlapping date range for the tables in the Data Lake where\n",
    "        - MaxTemp is not null\n",
    "        - Quality is equals to 'Y'\n",
    "    4. Filter each of the tables in the Data Lake where\n",
    "        - DateTime is between the date range calculated above\n",
    "        - MaxTemp is not null\n",
    "        - Quality is equals to 'Y'\n",
    "    5. Join the filtered tables on date creating a table with the following columns\n",
    "        - Year\n",
    "        - Month\n",
    "        - Day\n",
    "        - Airport_MaxTemp\n",
    "        - Gosnells_MaxTemp\n",
    "        - Swanbourne_MaxTemp\n",
    "        - Perth_MaxTemp\n",
    "        - Hillarys_MaxTemp\n",
    "    6. View the stats on the joined table using the describe method\n",
    "    7. Calculate a correlation matrix of the MaxTemp column in the joined table\n",
    "    8. View the correlation matrix as a heat map\n",
    "    9. Calculate a series of linear regressions where Perth_MaxTemp is the dependent variable/label\n",
    "        - Features: Airport\n",
    "        - Features: Airport,Gosnells\n",
    "        - Features: Airport,Gosnells,Swanbourne\n",
    "        - Features: Airport,Swanbourne\n",
    "        - Features: Airport,Gosnells,Swanbourne,Hillarys\n",
    "    10. Export the joined table into a new delta table in the data lake"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuro_python.neuro_compute import spark_manager as spm\n",
    "from neuro_python.neuro_data import schema_manager as sm\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "import plotly.graph_objs as go\n",
    "init_notebook_mode(connected=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start the default cluster and ensure it's running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spm.start_cluster()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spm.list_clusters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create context once cluster is in running state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spm.create_context('TrainingContext')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ImportData\n",
    "Change the datalake name and sql store name to match yours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark_import_table\n",
    "import_table('df_9021','DataLakeName','D3S_Training_9021_MaxTemp')\n",
    "import_table('df_9106','DataLakeName','D3S_Training_9106_MaxTemp')\n",
    "import_table('df_9215','DataLakeName','D3S_Training_9215_MaxTemp')\n",
    "import_table('df_9225','DataLakeName','D3S_Training_9225_MaxTemp')\n",
    "import_table('df_9265','DataLakeName','D3S_Training_9265_MaxTemp')\n",
    "import_table('df_Stations','SqlStoreName','D3S_Training_WeatherStations')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. View the top of each of the tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark_sql\n",
    "select * \n",
    "from df_9021\n",
    "limit 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark_sql\n",
    "select * \n",
    "from df_9106\n",
    "limit 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark_sql\n",
    "select * \n",
    "from df_9215\n",
    "limit 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark_sql\n",
    "select * \n",
    "from df_9225\n",
    "limit 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark_sql\n",
    "select * \n",
    "from df_9265\n",
    "limit 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark_sql\n",
    "select * \n",
    "from df_Stations\n",
    "limit 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Find the overlapping date range for the tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark\n",
    "import datetime\n",
    "from pyspark.sql.types import TimestampType\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.functions import min, max, first\n",
    "spark.udf.register('udf_myFunction1',datetime.datetime, TimestampType())\n",
    "udf_myFunction1 = udf(datetime.datetime, TimestampType())\n",
    "df2=df_9021.filter(\"MaxTemp is not null and Quality='Y'\").withColumn('Date',udf_myFunction1('Year','Month','Day')).select(min('Date'),max('Date')).limit(1)\n",
    "df2=df2.union(df_9106.filter(\"MaxTemp is not null and Quality='Y'\").withColumn('Date',udf_myFunction1('Year','Month','Day')).select(min('Date'),max('Date')).limit(1))\n",
    "df2=df2.union(df_9215.filter(\"MaxTemp is not null and Quality='Y'\").withColumn('Date',udf_myFunction1('Year','Month','Day')).select(min('Date'),max('Date')).limit(1))\n",
    "df2=df2.union(df_9225.filter(\"MaxTemp is not null and Quality='Y'\").withColumn('Date',udf_myFunction1('Year','Month','Day')).select(min('Date'),max('Date')).limit(1))\n",
    "df2=df2.union(df_9265.filter(\"MaxTemp is not null and Quality='Y'\").withColumn('Date',udf_myFunction1('Year','Month','Day')).select(min('Date'),max('Date')).limit(1))\n",
    "df2=df2.select(max('min(Date)'),min('max(Date)'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%spark_pandas -df df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Filter each of the tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark_sql -df df_9021_filtered\n",
    "select Year,Month,Day,MaxTemp\n",
    "from df_9021\n",
    "where Quality='Y'\n",
    "and udf_myFunction1(Year,Month,Day)>=udf_myFunction1(2013,5,16)\n",
    "and udf_myFunction1(Year,Month,Day)<=udf_myFunction1(2016,2,29)\n",
    "and MaxTemp is not null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark_sql -df df_9106_filtered\n",
    "select Year,Month,Day,MaxTemp\n",
    "from df_9106\n",
    "where Quality='Y'\n",
    "and udf_myFunction1(Year,Month,Day)>=udf_myFunction1(2013,5,16)\n",
    "and udf_myFunction1(Year,Month,Day)<=udf_myFunction1(2016,2,29)\n",
    "and MaxTemp is not null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark_sql -df df_9215_filtered\n",
    "select Year,Month,Day,MaxTemp\n",
    "from df_9215\n",
    "where Quality='Y'\n",
    "and udf_myFunction1(Year,Month,Day)>=udf_myFunction1(2013,5,16)\n",
    "and udf_myFunction1(Year,Month,Day)<=udf_myFunction1(2016,2,29)\n",
    "and MaxTemp is not null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark_sql -df df_9225_filtered\n",
    "select Year,Month,Day,MaxTemp\n",
    "from df_9225\n",
    "where Quality='Y'\n",
    "and udf_myFunction1(Year,Month,Day)>=udf_myFunction1(2013,5,16)\n",
    "and udf_myFunction1(Year,Month,Day)<=udf_myFunction1(2016,2,29)\n",
    "and MaxTemp is not null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark_sql -df df_9265_filtered\n",
    "select Year,Month,Day,MaxTemp\n",
    "from df_9265\n",
    "where Quality='Y'\n",
    "and udf_myFunction1(Year,Month,Day)>=udf_myFunction1(2013,5,16)\n",
    "and udf_myFunction1(Year,Month,Day)<=udf_myFunction1(2016,2,29)\n",
    "and MaxTemp is not null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Join the filtered tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark_sql -df df_joined\n",
    "select air.Year, air.Month, air.Day, air.MaxTemp as Airport_MaxTemp, gos.MaxTemp as Gosnells_MaxTemp, \n",
    "    per.MaxTemp as Perth_MaxTemp, hill.MaxTemp as Hillarys_MaxTemp, swan.MaxTemp as Swanbourne_MaxTemp\n",
    "from df_9021_filtered air\n",
    "join df_9106_filtered gos on\n",
    "    gos.year = air.year\n",
    "    and gos.month = air.month\n",
    "    and gos.day = air.day\n",
    "join df_9225_filtered per on\n",
    "    per.year = air.year\n",
    "    and per.month = air.month\n",
    "    and per.day = air.day\n",
    "join df_9265_filtered hill on\n",
    "    hill.year = air.year\n",
    "    and hill.month = air.month\n",
    "    and hill.day = air.day\n",
    "join df_9215_filtered swan on\n",
    "    swan.year = air.year\n",
    "    and swan.month = air.month\n",
    "    and swan.day = air.day"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. View the stats on the joined table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%spark_pandas -df df_joined.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Calculate a correlation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"Airport_MaxTemp\",\"Gosnells_MaxTemp\",\"Swanbourne_MaxTemp\",\"Perth_MaxTemp\",\"Hillarys_MaxTemp\"],\n",
    "    outputCol=\"features\")\n",
    "output = assembler.transform(df_joined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark\n",
    "from pyspark.ml.stat import Correlation\n",
    "r1 = Correlation.corr(output, \"features\").head()\n",
    "rows = r1[0].toArray().tolist()\n",
    "df_corr = spark.createDataFrame(rows,[\"Airport_MaxTemp\",\"Gosnells_MaxTemp\",\"Swanbourne_MaxTemp\",\"Perth_MaxTemp\",\"Hillarys_MaxTemp\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Return the result into the notebook so you can plot it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%spark_pandas -df df_corr -o df_corr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. View the correlation matrix as a heat map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iplot([go.Heatmap(z=df_corr.values.tolist(),\n",
    "                   x=[\"Airport_MaxTemp\",\"Gosnells_MaxTemp\",\"Swanbourne_MaxTemp\",\"Perth_MaxTemp\",\"Hillarys_MaxTemp\"],\n",
    "                   y=[\"Airport_MaxTemp\",\"Gosnells_MaxTemp\",\"Swanbourne_MaxTemp\",\"Perth_MaxTemp\",\"Hillarys_MaxTemp\"])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Calculate a series of linear regressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "lr = LinearRegression(labelCol='Perth_MaxTemp')\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"Airport_MaxTemp\"],\n",
    "    outputCol=\"features\")\n",
    "output = assembler.transform(df_joined)\n",
    "# Fit the model\n",
    "lrModel = lr.fit(output)\n",
    "\n",
    "# Print the coefficients and intercept for linear regression\n",
    "\"Coefficients: %s\" % str(lrModel.coefficients)+\" Intercept: %s\" % str(lrModel.intercept)+\" R-Squared: %s\"%lrModel.summary.r2+\" RMSE: %s\"%lrModel.summary.rootMeanSquaredError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This code won't work, it is an explaination of how to apply the linear regression results\n",
    "perth_est=airport*0.957775304263+0.8193198067865914"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark\n",
    "lr = LinearRegression(labelCol='Perth_MaxTemp')\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"Airport_MaxTemp\",\"Gosnells_MaxTemp\"],\n",
    "    outputCol=\"features\")\n",
    "output = assembler.transform(df_joined)\n",
    "# Fit the model\n",
    "lrModel = lr.fit(output)\n",
    "\n",
    "# Print the coefficients and intercept for linear regression\n",
    "\"Coefficients: %s\" % str(lrModel.coefficients)+\" Intercept: %s\" % str(lrModel.intercept)+\" R-Squared: %s\"%lrModel.summary.r2+\" RMSE: %s\"%lrModel.summary.rootMeanSquaredError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark\n",
    "lr = LinearRegression(labelCol='Perth_MaxTemp')\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"Airport_MaxTemp\",\"Gosnells_MaxTemp\",\"Swanbourne_MaxTemp\"],\n",
    "    outputCol=\"features\")\n",
    "output = assembler.transform(df_joined)\n",
    "# Fit the model\n",
    "lrModel = lr.fit(output)\n",
    "\n",
    "# Print the coefficients and intercept for linear regression\n",
    "\"Coefficients: %s\" % str(lrModel.coefficients)+\" Intercept: %s\" % str(lrModel.intercept)+\" R-Squared: %s\"%lrModel.summary.r2+\" RMSE: %s\"%lrModel.summary.rootMeanSquaredError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark\n",
    "lr = LinearRegression(labelCol='Perth_MaxTemp')\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"Airport_MaxTemp\",\"Swanbourne_MaxTemp\"],\n",
    "    outputCol=\"features\")\n",
    "output = assembler.transform(df_joined)\n",
    "# Fit the model\n",
    "lrModel = lr.fit(output)\n",
    "\n",
    "# Print the coefficients and intercept for linear regression\n",
    "\"Coefficients: %s\" % str(lrModel.coefficients)+\" Intercept: %s\" % str(lrModel.intercept)+\" R-Squared: %s\"%lrModel.summary.r2+\" RMSE: %s\"%lrModel.summary.rootMeanSquaredError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark\n",
    "lr = LinearRegression(labelCol='Perth_MaxTemp')\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"Airport_MaxTemp\",\"Gosnells_MaxTemp\",\"Swanbourne_MaxTemp\",\"Hillarys_MaxTemp\"],\n",
    "    outputCol=\"features\")\n",
    "output = assembler.transform(df_joined)\n",
    "# Fit the model\n",
    "lrModel = lr.fit(output)\n",
    "\n",
    "# Print the coefficients and intercept for linear regression\n",
    "\"Coefficients: %s\" % str(lrModel.coefficients)+\" Intercept: %s\" % str(lrModel.intercept)+\" R-Squared: %s\"%lrModel.summary.r2+\" RMSE: %s\"%lrModel.summary.rootMeanSquaredError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Export the joined table into a new delta table in the data lake\n",
    "Change the data lake name and table name to match yours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols=[sm.column_definition('Year','Int'),\n",
    "     sm.column_definition('Month','Int'),\n",
    "     sm.column_definition('Day','Int'),\n",
    "     sm.column_definition('Airport_MaxTemp','Double'),\n",
    "     sm.column_definition('Gosnells_MaxTemp','Double'),\n",
    "     sm.column_definition('Swanbourne_MaxTemp','Double'),\n",
    "     sm.column_definition('Perth_MaxTemp','Double'),\n",
    "     sm.column_definition('Hillarys_MaxTemp','Double')]\n",
    "table_def=sm.table_definition(cols,'Processed',file_type='delta')\n",
    "sm.create_table('DataLakeName','D3S_Training_Lee_PerthMaxTemps',table_def)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark_export_table\n",
    "export_table('df_joined','DataLakeName','D3S_Training_Lee_PerthMaxTemps')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test that the data exported correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark_import_table\n",
    "import_table('testdata','DataLakeName','D3S_Training_Lee_PerthMaxTemps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark_sql\n",
    "select *\n",
    "from testdata\n",
    "limit 10"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
